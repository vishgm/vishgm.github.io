<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Transformer Architecture | Vishak Gopkumar</title>
    
    
    <link rel="icon" type="image/png" href="https://github.com/vishgm.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://github.com/vishgm.png" sizes="16x16">
    <link rel="apple-touch-icon" href="https://github.com/vishgm.png">
    
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&display=swap" rel="stylesheet">
    
    
    
    <link rel="stylesheet" href="/css/main.css">
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false}
                ],
                throwOnError : false
            });
        });
    </script>
    
    <script>
        
        const theme = localStorage.getItem('theme') || 
                     (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light');
        document.documentElement.setAttribute('data-theme', theme);
    </script>
</head>
<body>
    <div class="container">
        <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" class="logo-link" aria-label="Home">
                
                <img src="https://github.com/vishgm.png" alt="Vishak Gopkumar" class="nav-avatar">
                
                <span class="nav-title">Vishak Gopkumar</span>
            </a>
        </div>
        <div class="nav-links">
            
            <a href="/" class="nav-link">Home</a>
            
            <a href="/posts/" class="nav-link">Posts</a>
            
            <a href="/about/" class="nav-link">About</a>
            
            <button id="theme-toggle" class="theme-toggle" aria-label="Toggle theme">
                <svg class="theme-toggle-icon sun-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <circle cx="12" cy="12" r="5"></circle>
                    <line x1="12" y1="1" x2="12" y2="3"></line>
                    <line x1="12" y1="21" x2="12" y2="23"></line>
                    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                    <line x1="1" y1="12" x2="3" y2="12"></line>
                    <line x1="21" y1="12" x2="23" y2="12"></line>
                    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                </svg>
                <svg class="theme-toggle-icon moon-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                </svg>
            </button>
        </div>
    </nav>
</header> 
        <main>
            
<div class="post-navigation-top">
    <div class="breadcrumbs">
        <a href="/">Home</a>
        
        <span class="separator">/</span>
        <a href="/ml">Ml</a>
        
        <span class="separator">/</span>
        <span class="current">Understanding Transformer Architecture</span>
    </div>
    <a href="/posts" class="back-to-posts">← Back to all posts</a>
</div>

<article class="post">
    <header class="post-header">
        <h1 class="post-title">Understanding Transformer Architecture</h1>
        <div class="post-meta">
            <time>Mar 20, 2024</time>
            
            <span class="reading-time">1 min read</span>
            
        </div>
    </header>

    
    
    <div class="table-of-contents">
        <h2>Table of Contents</h2>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#key-components">Key Components</a>
      <ul>
        <li><a href="#1-self-attention-mechanism">1. Self-Attention Mechanism</a></li>
        <li><a href="#2-multi-head-attention">2. Multi-Head Attention</a></li>
      </ul>
    </li>
    <li><a href="#why-transformers-matter">Why Transformers Matter</a></li>
    <li><a href="#further-reading">Further Reading</a></li>
  </ul>
</nav>
    </div>
    
    

    <div class="post-content">
        <p>The transformer architecture has revolutionized natural language processing since its introduction in the &ldquo;Attention Is All You Need&rdquo; paper. Let&rsquo;s break down its key components.</p>
<h2 id="key-components">Key Components</h2>
<h3 id="1-self-attention-mechanism">1. Self-Attention Mechanism</h3>
<p>The heart of the transformer is its self-attention mechanism. It allows the model to weigh the importance of different words in the input sequence:</p>
<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>
<h3 id="2-multi-head-attention">2. Multi-Head Attention</h3>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff79c6">class</span> <span style="color:#50fa7b">MultiHeadAttention</span>(nn<span style="color:#ff79c6">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">def</span> __init__(self, d_model, num_heads):
</span></span><span style="display:flex;"><span>        <span style="color:#8be9fd;font-style:italic">super</span>()<span style="color:#ff79c6">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#ff79c6">.</span>num_heads <span style="color:#ff79c6">=</span> num_heads
</span></span><span style="display:flex;"><span>        self<span style="color:#ff79c6">.</span>d_model <span style="color:#ff79c6">=</span> d_model
</span></span><span style="display:flex;"><span>        self<span style="color:#ff79c6">.</span>d_k <span style="color:#ff79c6">=</span> d_model <span style="color:#ff79c6">//</span> num_heads
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#ff79c6">.</span>W_q <span style="color:#ff79c6">=</span> nn<span style="color:#ff79c6">.</span>Linear(d_model, d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#ff79c6">.</span>W_k <span style="color:#ff79c6">=</span> nn<span style="color:#ff79c6">.</span>Linear(d_model, d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#ff79c6">.</span>W_v <span style="color:#ff79c6">=</span> nn<span style="color:#ff79c6">.</span>Linear(d_model, d_model)
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="why-transformers-matter">Why Transformers Matter</h2>
<ol>
<li><strong>Parallelization</strong>: Unlike RNNs, transformers can process all input tokens simultaneously</li>
<li><strong>Long-range Dependencies</strong>: Self-attention helps capture relationships between distant words</li>
<li><strong>Scalability</strong>: The architecture scales well with more data and compute</li>
</ol>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
</ul>

    </div>

    
    <nav class="post-navigation">
        
        <a class="prev-post" href="/ml/first-ml-post/">← Getting Started with Machine Learning</a>
        
        
    </nav>
    
</article>

        </main>
        <footer class="footer">
    <div class="footer-content">
        <p>&copy; 2025 Vishak Gopkumar</p>
    </div>
</footer> 
    </div>
    
    <script src="/js/main.min.js"></script>
    
    
    <button id="scroll-to-top" class="scroll-to-top" aria-label="Scroll to top">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <line x1="12" y1="19" x2="12" y2="5"></line>
            <polyline points="5 12 12 5 19 12"></polyline>
        </svg>
    </button>
</body>
</html> 