<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on Vishak Gopkumar</title><link>http://localhost:1313/ml/</link><description>Recent content in Machine Learning on Vishak Gopkumar</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 20 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Getting Started with Machine Learning</title><link>http://localhost:1313/ml/first-ml-post/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/ml/first-ml-post/</guid><description>&lt;p>This is my first post in the Machine Learning category. Here I&amp;rsquo;ll be sharing my learning journey, insights, and experiences in the field of ML.&lt;/p></description></item><item><title>Understanding Transformer Architecture</title><link>http://localhost:1313/ml/understanding-transformers/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>http://localhost:1313/ml/understanding-transformers/</guid><description>&lt;p>The transformer architecture has revolutionized natural language processing since its introduction in the &amp;ldquo;Attention Is All You Need&amp;rdquo; paper. Let&amp;rsquo;s break down its key components.&lt;/p>
&lt;h2 id="key-components">Key Components&lt;/h2>
&lt;h3 id="1-self-attention-mechanism">1. Self-Attention Mechanism&lt;/h3>
&lt;p>The heart of the transformer is its self-attention mechanism. It allows the model to weigh the importance of different words in the input sequence:&lt;/p>
&lt;p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$&lt;/p>
&lt;h3 id="2-multi-head-attention">2. Multi-Head Attention&lt;/h3>
&lt;div class="highlight">&lt;div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
&lt;table style="border-spacing:0;padding:0;margin:0;border:0;">&lt;tr>&lt;td style="vertical-align:top;padding:0;margin:0;border:0;">
&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
&lt;/span>&lt;span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ff79c6">class&lt;/span> &lt;span style="color:#50fa7b">MultiHeadAttention&lt;/span>(nn&lt;span style="color:#ff79c6">.&lt;/span>Module):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ff79c6">def&lt;/span> __init__(self, d_model, num_heads):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#8be9fd;font-style:italic">super&lt;/span>()&lt;span style="color:#ff79c6">.&lt;/span>__init__()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff79c6">.&lt;/span>num_heads &lt;span style="color:#ff79c6">=&lt;/span> num_heads
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff79c6">.&lt;/span>d_model &lt;span style="color:#ff79c6">=&lt;/span> d_model
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff79c6">.&lt;/span>d_k &lt;span style="color:#ff79c6">=&lt;/span> d_model &lt;span style="color:#ff79c6">//&lt;/span> num_heads
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff79c6">.&lt;/span>W_q &lt;span style="color:#ff79c6">=&lt;/span> nn&lt;span style="color:#ff79c6">.&lt;/span>Linear(d_model, d_model)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff79c6">.&lt;/span>W_k &lt;span style="color:#ff79c6">=&lt;/span> nn&lt;span style="color:#ff79c6">.&lt;/span>Linear(d_model, d_model)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> self&lt;span style="color:#ff79c6">.&lt;/span>W_v &lt;span style="color:#ff79c6">=&lt;/span> nn&lt;span style="color:#ff79c6">.&lt;/span>Linear(d_model, d_model)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="why-transformers-matter">Why Transformers Matter&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Parallelization&lt;/strong>: Unlike RNNs, transformers can process all input tokens simultaneously&lt;/li>
&lt;li>&lt;strong>Long-range Dependencies&lt;/strong>: Self-attention helps capture relationships between distant words&lt;/li>
&lt;li>&lt;strong>Scalability&lt;/strong>: The architecture scales well with more data and compute&lt;/li>
&lt;/ol>
&lt;h2 id="further-reading">Further Reading&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>