<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Vishak Gopkumar</title>
    <link>http://localhost:1313/ml/</link>
    <description>Recent content in Machine Learning on Vishak Gopkumar</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Feb 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Getting Started with Machine Learning</title>
      <link>http://localhost:1313/ml/first-ml-post/</link>
      <pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/ml/first-ml-post/</guid>
      <description>&lt;p&gt;This is my first post in the Machine Learning category. Here I&amp;rsquo;ll be sharing my learning journey, insights, and experiences in the field of ML.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding Transformer Architecture</title>
      <link>http://localhost:1313/ml/understanding-transformers/</link>
      <pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/ml/understanding-transformers/</guid>
      <description>&lt;p&gt;The transformer architecture has revolutionized natural language processing since its introduction in the &amp;ldquo;Attention Is All You Need&amp;rdquo; paper. Let&amp;rsquo;s break down its key components.&lt;/p&gt;&#xA;&lt;h2 id=&#34;key-components&#34;&gt;Key Components&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-self-attention-mechanism&#34;&gt;1. Self-Attention Mechanism&lt;/h3&gt;&#xA;&lt;p&gt;The heart of the transformer is its self-attention mechanism. It allows the model to weigh the importance of different words in the input sequence:&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V&#xA;$$&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-multi-head-attention&#34;&gt;2. Multi-Head Attention&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;div style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&#xA;&lt;table style=&#34;border-spacing:0;padding:0;margin:0;border:0;&#34;&gt;&lt;tr&gt;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;&#34;&gt;&#xA;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 1&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 2&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 3&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 4&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 5&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 6&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 7&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 8&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt; 9&#xA;&lt;/span&gt;&lt;span style=&#34;white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f&#34;&gt;10&#xA;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&#xA;&lt;td style=&#34;vertical-align:top;padding:0;margin:0;border:0;;width:100%&#34;&gt;&#xA;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;MultiHeadAttention&lt;/span&gt;(nn&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Module):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; __init__(self, d_model, num_heads):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;super&lt;/span&gt;()&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;__init__()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;num_heads &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; num_heads&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;d_model &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; d_model&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;d_k &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; d_model &lt;span style=&#34;color:#ff79c6&#34;&gt;//&lt;/span&gt; num_heads&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;W_q &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Linear(d_model, d_model)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;W_k &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Linear(d_model, d_model)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;W_v &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Linear(d_model, d_model)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&#xA;&lt;/div&gt;&#xA;&lt;/div&gt;&lt;h2 id=&#34;why-transformers-matter&#34;&gt;Why Transformers Matter&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Parallelization&lt;/strong&gt;: Unlike RNNs, transformers can process all input tokens simultaneously&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Long-range Dependencies&lt;/strong&gt;: Self-attention helps capture relationships between distant words&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: The architecture scales well with more data and compute&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;further-reading&#34;&gt;Further Reading&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34;&gt;The Illustrated Transformer&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
